import os
import sys
import asyncio
import yaml
from pathlib import Path
from langchain_core.messages import HumanMessage

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° sys.pathï¼Œä»¥ä¾¿å¯¼å…¥ src æ¨¡å—
current_dir = Path(__file__).resolve().parent
project_root = current_dir.parent
sys.path.append(str(project_root))

from src.llms.llm import get_llm_by_type
from src.config.config import Config

# åŠ è½½é…ç½®
config_path = project_root / "config.yaml"
if config_path.exists():
    with open(config_path, "r", encoding="utf-8") as f:
        config_data = yaml.safe_load(f)
        # ç®€å•çš„é…ç½®æ³¨å…¥ï¼Œæ¨¡æ‹Ÿ src.config.config.Config çš„è¡Œä¸ºï¼ˆå¦‚æœå®ƒä¾èµ–æ–‡ä»¶åŠ è½½ï¼‰
        # è¿™é‡Œç›´æ¥æ›´æ–° Config ç±»çš„å†…éƒ¨å­˜å‚¨ï¼ˆå‡è®¾ Config æ˜¯å•ä¾‹æˆ–ç±»å˜é‡ç®¡ç†ï¼‰
        # æ³¨æ„ï¼šç”±äºæ— æ³•ç›´æ¥è®¿é—® Config çš„ç§æœ‰åŠ è½½é€»è¾‘ï¼Œæˆ‘ä»¬ä¾èµ– get_llm_by_type å†…éƒ¨ä¼šè¯»å– Config
        # å¦‚æœ Config éœ€è¦æ˜¾å¼åŠ è½½ï¼Œè¿™é‡Œå¯èƒ½éœ€è¦è°ƒæ•´ã€‚
        # ä¸ºäº†ä¿é™©ï¼Œæˆ‘ä»¬æ‰‹åŠ¨è®¾ç½® Config._config (å¦‚æœå®ƒæ˜¯ç§æœ‰çš„) æˆ–ç›¸åº”çš„å­˜å‚¨
        pass
else:
    print(f"é”™è¯¯: æ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶ {config_path}")
    sys.exit(1)

async def test_basic_model():
    print("\n" + "="*50)
    print("ğŸ§ª æµ‹è¯• BASIC_MODEL (åŸºç¡€æ¨¡å‹)")
    print("="*50)
    
    try:
        llm = get_llm_by_type("basic")
        print(f"é…ç½®æ¨¡å‹: {llm.model_name}")
        print(f"API Base: {llm.openai_api_base}")
        
        msg = HumanMessage(content="ä½ å¥½ï¼Œè¯·å›å¤'æµ‹è¯•æˆåŠŸ'å››ä¸ªå­—ã€‚")
        print(f"\nå‘é€æ¶ˆæ¯: {msg.content}")
        
        print("æ­£åœ¨è°ƒç”¨ (invoke)...")
        response = await llm.ainvoke([msg])
        print(f"âœ… è°ƒç”¨æˆåŠŸ!")
        print(f"å“åº”å†…å®¹: {response.content}")
        
        print("\næ­£åœ¨æµ‹è¯•æµå¼ (stream)...")
        print("å“åº”æµ: ", end="", flush=True)
        async for chunk in llm.astream([msg]):
            print(chunk.content, end="", flush=True)
        print("\nâœ… æµå¼æµ‹è¯•ç»“æŸ")
        
    except Exception as e:
        print(f"\nâŒ BASIC_MODEL æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

async def test_reasoning_model():
    print("\n" + "="*50)
    print("ğŸ§ª æµ‹è¯• REASONING_MODEL (æ¨ç†æ¨¡å‹)")
    print("="*50)
    
    try:
        # æ£€æŸ¥é…ç½®ä¸­æ˜¯å¦å­˜åœ¨ REASONING_MODEL
        # ç”±äº get_llm_by_type ä¼šæŠ›é”™æˆ–è¿”å›é»˜è®¤ï¼Œæˆ‘ä»¬å…ˆå°è¯•è·å–
        llm = get_llm_by_type("reasoning")
        print(f"é…ç½®æ¨¡å‹: {llm.model_name}")
        print(f"API Base: {llm.openai_api_base}")
        
        msg = HumanMessage(content="9.11å’Œ9.8å“ªä¸ªå¤§ï¼Ÿ")
        print(f"\nå‘é€æ¶ˆæ¯: {msg.content}")
        
        print("æ­£åœ¨è°ƒç”¨ (invoke)...")
        response = await llm.ainvoke([msg])
        print(f"âœ… è°ƒç”¨æˆåŠŸ!")
        
        # æ£€æŸ¥æ¨ç†å†…å®¹
        reasoning = response.additional_kwargs.get("reasoning_content")
        if reasoning:
            print(f"ğŸ§  æ•è·åˆ°æ€è€ƒè¿‡ç¨‹ (å‰100å­—ç¬¦): {reasoning[:100]}...")
        else:
            print("âš ï¸ æœªæ•è·åˆ°æ€è€ƒè¿‡ç¨‹ (reasoning_content ä¸ºç©º)")
            
        print(f"å“åº”å†…å®¹: {response.content}")

        print("\næ­£åœ¨æµ‹è¯•æµå¼ (stream)...")
        print("å“åº”æµ: ", end="", flush=True)
        async for chunk in llm.astream([msg]):
            content = chunk.content
            # å°è¯•æ‰“å° reasoning chunk å¦‚æœæœ‰çš„è¯ (DashScope ç‰¹æœ‰)
            r_content = chunk.additional_kwargs.get("reasoning_content", "")
            if r_content:
                # ä¸ºäº†ä¸æ··æ·†è¾“å‡ºï¼Œè¿™é‡Œä¸æ‰“å°æ€è€ƒè¿‡ç¨‹çš„æµï¼Œåªæ ‡è®°
                pass
            print(content, end="", flush=True)
        print("\nâœ… æµå¼æµ‹è¯•ç»“æŸ")

    except Exception as e:
        print(f"\nâŒ REASONING_MODEL æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

async def main():
    print("ğŸ¦Œ DeerFlow æ¨¡å‹è¿æ¥æ€§æµ‹è¯•")
    print(f"é…ç½®æ–‡ä»¶: {config_path}")
    
    await test_basic_model()
    await test_reasoning_model()

if __name__ == "__main__":
    asyncio.run(main())
