import json
import sys
from pathlib import Path

# å°†é¡¹ç›®æ ¹ç›®å½•åŠ å…¥ sys.pathï¼Œç¡®ä¿å¯ä»¥å¯¼å…¥ src æ¨¡å—
current_dir = Path(__file__).resolve().parent
project_root = current_dir.parent
if str(project_root) not in sys.path:
    sys.path.append(str(project_root))

from src.tools.crawl import crawl_tool  # noqa: E402


def run_crawl_tool_smoke_test() -> None:
    """å¯¹ crawl_tool åšä¸€æ¬¡æœ€ç®€å•çš„å†’çƒŸæµ‹è¯•ã€‚

    - å®é™…è°ƒç”¨ Jina + Readability + Crawler é“¾è·¯
    - ä½¿ç”¨ä¸€ä¸ªç¨³å®šçš„å…¬å…±ç½‘é¡µ URL
    - æ–­è¨€è¿”å›ç»“æœæ˜¯ JSON å­—ç¬¦ä¸²ï¼Œä¸”åŒ…å« url å’Œ crawled_content å­—æ®µ
    """
    test_url = "https://baijiahao.baidu.com/s?id=1833164367864050283&wfr=spider&for=pc"
    print("\n" + "=" * 50)
    print("ğŸ§ª æµ‹è¯• crawl_tool")
    print("=" * 50)
    print(f"æµ‹è¯• URL: {test_url}")

    try:
        # è°ƒç”¨ LangChain å·¥å…·ï¼ˆåŒæ­¥ï¼‰
        raw_result = crawl_tool.invoke(test_url)
        print("\nåŸå§‹è¿”å›ï¼ˆå‰ 400 å­—ç¬¦ï¼‰:\n")
        preview = raw_result[:400]
        print(preview + ("..." if len(raw_result) > 400 else ""))

        # è§£æ JSON
        data = json.loads(raw_result)
        if not isinstance(data, dict):
            raise ValueError("crawl_tool è¿”å›çš„ JSON é¡¶å±‚ä¸æ˜¯å¯¹è±¡")

        url = data.get("url")
        crawled_content = data.get("crawled_content")

        if url != test_url:
            raise ValueError(f"è¿”å›çš„ url ä¸åŒ¹é…ï¼ŒæœŸæœ› {test_url}ï¼Œå®é™… {url}")

        if not isinstance(crawled_content, str) or not crawled_content.strip():
            raise ValueError("crawled_content ä¸ºç©ºæˆ–ä¸æ˜¯å­—ç¬¦ä¸²")

        print("\nâœ… crawl_tool å†’çƒŸæµ‹è¯•é€šè¿‡ï¼")
        print(f"crawled_content é•¿åº¦: {len(crawled_content)} å­—ç¬¦")
    except Exception as e:  # pylint: disable=broad-except
        print(f"\nâŒ crawl_tool æµ‹è¯•å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    run_crawl_tool_smoke_test()
